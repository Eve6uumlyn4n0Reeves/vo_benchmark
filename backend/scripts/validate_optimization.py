#!/usr/bin/env python3
"""
ä¼˜åŒ–éªŒè¯è„šæœ¬ï¼šéªŒè¯è½¨è¿¹åŠ è½½ä¼˜åŒ–æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import argparse
import asyncio
import time
import sys
from pathlib import Path
import json
import aiohttp
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.storage.experiment import ExperimentStorage

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class OptimizationValidator:
    """ä¼˜åŒ–éªŒè¯å™¨"""
    
    def __init__(self, base_url: str = "http://127.0.0.1:5000"):
        self.base_url = base_url.rstrip('/')
        self.session = None
        
        # æµ‹è¯•ç»“æœ
        self.results = {
            'manifest_tests': [],
            'trajectory_tests': [],
            'pr_curve_tests': [],
            'performance_tests': [],
            'errors': []
        }
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def test_manifest_endpoint(self, experiment_id: str, algorithm_key: str) -> dict:
        """æµ‹è¯•æ¸…å•ç«¯ç‚¹"""
        test_result = {
            'test': 'manifest',
            'experiment_id': experiment_id,
            'algorithm_key': algorithm_key,
            'success': False,
            'response_time': 0,
            'payload_size': 0,
            'has_trajectory': False,
            'has_pr_curve': False,
            'error': None
        }
        
        try:
            start_time = time.time()
            
            url = f"{self.base_url}/api/v1/results/{experiment_id}/{algorithm_key}/manifest"
            async with self.session.get(url) as response:
                test_result['response_time'] = time.time() - start_time
                
                if response.status == 200:
                    data = await response.json()
                    test_result['success'] = True
                    test_result['payload_size'] = len(await response.text())
                    
                    # æ£€æŸ¥æ¸…å•å†…å®¹
                    test_result['has_trajectory'] = bool(data.get('trajectory'))
                    test_result['has_pr_curve'] = bool(data.get('pr_curve'))
                    
                    logger.info(f"âœ… Manifestæµ‹è¯•é€šè¿‡: {experiment_id}/{algorithm_key} ({test_result['response_time']:.3f}s)")
                else:
                    test_result['error'] = f"HTTP {response.status}"
                    logger.warning(f"âŒ Manifestæµ‹è¯•å¤±è´¥: {experiment_id}/{algorithm_key} - {test_result['error']}")
                    
        except Exception as e:
            test_result['error'] = str(e)
            test_result['response_time'] = time.time() - start_time
            logger.error(f"âŒ Manifestæµ‹è¯•å¼‚å¸¸: {experiment_id}/{algorithm_key} - {e}")
        
        self.results['manifest_tests'].append(test_result)
        return test_result
    
    async def test_trajectory_endpoint(self, experiment_id: str, algorithm_key: str, max_points: str = "1500") -> dict:
        """æµ‹è¯•è½¨è¿¹ç«¯ç‚¹"""
        test_result = {
            'test': 'trajectory',
            'experiment_id': experiment_id,
            'algorithm_key': algorithm_key,
            'max_points': max_points,
            'success': False,
            'response_time': 0,
            'payload_size': 0,
            'point_count': 0,
            'compressed': False,
            'error': None
        }
        
        try:
            start_time = time.time()
            
            url = f"{self.base_url}/api/v1/results/{experiment_id}/{algorithm_key}/trajectory"
            params = {'max_points': max_points} if max_points != "1500" else {}
            
            async with self.session.get(url, params=params) as response:
                test_result['response_time'] = time.time() - start_time
                
                # æ£€æŸ¥å“åº”å¤´
                test_result['compressed'] = 'gzip' in response.headers.get('Content-Encoding', '')
                
                if response.status == 200:
                    data = await response.json()
                    test_result['success'] = True
                    test_result['payload_size'] = len(await response.text())
                    
                    # è®¡ç®—ç‚¹æ•°
                    estimated = data.get('estimated_trajectory', data.get('poses', []))
                    test_result['point_count'] = len(estimated)
                    
                    logger.info(f"âœ… è½¨è¿¹æµ‹è¯•é€šè¿‡: {experiment_id}/{algorithm_key} ({test_result['point_count']} ç‚¹, {test_result['response_time']:.3f}s)")
                else:
                    test_result['error'] = f"HTTP {response.status}"
                    logger.warning(f"âŒ è½¨è¿¹æµ‹è¯•å¤±è´¥: {experiment_id}/{algorithm_key} - {test_result['error']}")
                    
        except Exception as e:
            test_result['error'] = str(e)
            test_result['response_time'] = time.time() - start_time
            logger.error(f"âŒ è½¨è¿¹æµ‹è¯•å¼‚å¸¸: {experiment_id}/{algorithm_key} - {e}")
        
        self.results['trajectory_tests'].append(test_result)
        return test_result
    
    async def test_pr_curve_endpoint(self, experiment_id: str, algorithm_key: str) -> dict:
        """æµ‹è¯•PRæ›²çº¿ç«¯ç‚¹"""
        test_result = {
            'test': 'pr_curve',
            'experiment_id': experiment_id,
            'algorithm_key': algorithm_key,
            'success': False,
            'response_time': 0,
            'payload_size': 0,
            'point_count': 0,
            'has_raw_data': False,
            'compressed': False,
            'error': None
        }
        
        try:
            start_time = time.time()
            
            url = f"{self.base_url}/api/v1/results/{experiment_id}/{algorithm_key}/pr-curve"
            async with self.session.get(url) as response:
                test_result['response_time'] = time.time() - start_time
                
                # æ£€æŸ¥å“åº”å¤´
                test_result['compressed'] = 'gzip' in response.headers.get('Content-Encoding', '')
                
                if response.status == 200:
                    data = await response.json()
                    test_result['success'] = True
                    test_result['payload_size'] = len(await response.text())
                    
                    # æ£€æŸ¥æ•°æ®
                    test_result['point_count'] = len(data.get('precisions', []))
                    test_result['has_raw_data'] = 'raw_precisions' in data
                    
                    logger.info(f"âœ… PRæ›²çº¿æµ‹è¯•é€šè¿‡: {experiment_id}/{algorithm_key} ({test_result['point_count']} ç‚¹, raw: {test_result['has_raw_data']})")
                else:
                    test_result['error'] = f"HTTP {response.status}"
                    logger.warning(f"âŒ PRæ›²çº¿æµ‹è¯•å¤±è´¥: {experiment_id}/{algorithm_key} - {test_result['error']}")
                    
        except Exception as e:
            test_result['error'] = str(e)
            test_result['response_time'] = time.time() - start_time
            logger.error(f"âŒ PRæ›²çº¿æµ‹è¯•å¼‚å¸¸: {experiment_id}/{algorithm_key} - {e}")
        
        self.results['pr_curve_tests'].append(test_result)
        return test_result
    
    async def test_performance_comparison(self, experiment_id: str, algorithm_key: str) -> dict:
        """æµ‹è¯•æ€§èƒ½å¯¹æ¯”ï¼ˆUI vs Fullï¼‰"""
        test_result = {
            'test': 'performance',
            'experiment_id': experiment_id,
            'algorithm_key': algorithm_key,
            'ui_time': 0,
            'full_time': 0,
            'ui_size': 0,
            'full_size': 0,
            'ui_points': 0,
            'full_points': 0,
            'improvement_ratio': 0,
            'error': None
        }
        
        try:
            # æµ‹è¯•UIç‰ˆæœ¬
            ui_result = await self.test_trajectory_endpoint(experiment_id, algorithm_key, "1500")
            if ui_result['success']:
                test_result['ui_time'] = ui_result['response_time']
                test_result['ui_size'] = ui_result['payload_size']
                test_result['ui_points'] = ui_result['point_count']
            
            # æµ‹è¯•Fullç‰ˆæœ¬
            full_result = await self.test_trajectory_endpoint(experiment_id, algorithm_key, "full")
            if full_result['success']:
                test_result['full_time'] = full_result['response_time']
                test_result['full_size'] = full_result['payload_size']
                test_result['full_points'] = full_result['point_count']
            
            # è®¡ç®—æ”¹è¿›æ¯”ä¾‹
            if test_result['full_time'] > 0:
                test_result['improvement_ratio'] = test_result['full_time'] / test_result['ui_time']
            
            logger.info(f"ğŸ“Š æ€§èƒ½å¯¹æ¯”: UI({test_result['ui_points']}ç‚¹, {test_result['ui_time']:.3f}s) vs Full({test_result['full_points']}ç‚¹, {test_result['full_time']:.3f}s)")
            
        except Exception as e:
            test_result['error'] = str(e)
            logger.error(f"âŒ æ€§èƒ½æµ‹è¯•å¼‚å¸¸: {experiment_id}/{algorithm_key} - {e}")
        
        self.results['performance_tests'].append(test_result)
        return test_result
    
    def discover_test_cases(self, storage_root: str) -> list:
        """å‘ç°æµ‹è¯•ç”¨ä¾‹"""
        storage = ExperimentStorage(storage_root)
        test_cases = []
        
        experiments_dir = Path(storage_root) / "experiments"
        if not experiments_dir.exists():
            logger.warning(f"å®éªŒç›®å½•ä¸å­˜åœ¨: {experiments_dir}")
            return test_cases
        
        for exp_dir in experiments_dir.iterdir():
            if not exp_dir.is_dir():
                continue
            
            experiment_id = exp_dir.name
            
            # æŸ¥æ‰¾ç®—æ³•
            algorithms = set()
            
            # ä»è½¨è¿¹ç›®å½•å‘ç°
            traj_dir = exp_dir / "trajectories"
            if traj_dir.exists():
                for file in traj_dir.glob("*.json.gz"):
                    algorithms.add(file.stem)
                for file in traj_dir.glob("*.arrow"):
                    if not file.name.endswith('.ui.arrow'):
                        algorithms.add(file.stem)
            
            # ä»PRæ›²çº¿ç›®å½•å‘ç°
            pr_dir = exp_dir / "pr_curves"
            if pr_dir.exists():
                for file in pr_dir.glob("*.json.gz"):
                    algorithms.add(file.stem)
                for file in pr_dir.glob("*.arrow"):
                    if not file.name.endswith('.ui.arrow'):
                        algorithms.add(file.stem)
            
            for algorithm_key in algorithms:
                test_cases.append((experiment_id, algorithm_key))
        
        logger.info(f"å‘ç° {len(test_cases)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return test_cases
    
    async def run_all_tests(self, storage_root: str, max_cases: int = 5) -> None:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        test_cases = self.discover_test_cases(storage_root)
        
        if not test_cases:
            logger.warning("æ²¡æœ‰å‘ç°æµ‹è¯•ç”¨ä¾‹")
            return
        
        # é™åˆ¶æµ‹è¯•ç”¨ä¾‹æ•°é‡
        test_cases = test_cases[:max_cases]
        logger.info(f"å°†æµ‹è¯• {len(test_cases)} ä¸ªç”¨ä¾‹")
        
        for i, (experiment_id, algorithm_key) in enumerate(test_cases, 1):
            logger.info(f"\n[{i}/{len(test_cases)}] æµ‹è¯•: {experiment_id}/{algorithm_key}")
            
            # æµ‹è¯•æ¸…å•
            await self.test_manifest_endpoint(experiment_id, algorithm_key)
            
            # æµ‹è¯•è½¨è¿¹
            await self.test_trajectory_endpoint(experiment_id, algorithm_key)
            
            # æµ‹è¯•PRæ›²çº¿
            await self.test_pr_curve_endpoint(experiment_id, algorithm_key)
            
            # æ€§èƒ½å¯¹æ¯”
            await self.test_performance_comparison(experiment_id, algorithm_key)
            
            # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡è½½
            await asyncio.sleep(0.1)
    
    def print_summary(self) -> None:
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ä¼˜åŒ–éªŒè¯æ‘˜è¦")
        print("="*60)
        
        # æ¸…å•æµ‹è¯•
        manifest_success = sum(1 for t in self.results['manifest_tests'] if t['success'])
        print(f"æ¸…å•æµ‹è¯•: {manifest_success}/{len(self.results['manifest_tests'])} é€šè¿‡")
        
        # è½¨è¿¹æµ‹è¯•
        trajectory_success = sum(1 for t in self.results['trajectory_tests'] if t['success'])
        print(f"è½¨è¿¹æµ‹è¯•: {trajectory_success}/{len(self.results['trajectory_tests'])} é€šè¿‡")
        
        # PRæ›²çº¿æµ‹è¯•
        pr_success = sum(1 for t in self.results['pr_curve_tests'] if t['success'])
        print(f"PRæ›²çº¿æµ‹è¯•: {pr_success}/{len(self.results['pr_curve_tests'])} é€šè¿‡")
        
        # æ€§èƒ½ç»Ÿè®¡
        if self.results['performance_tests']:
            avg_ui_time = sum(t['ui_time'] for t in self.results['performance_tests']) / len(self.results['performance_tests'])
            avg_full_time = sum(t['full_time'] for t in self.results['performance_tests'] if t['full_time'] > 0)
            if avg_full_time:
                avg_full_time /= len([t for t in self.results['performance_tests'] if t['full_time'] > 0])
                improvement = avg_full_time / avg_ui_time if avg_ui_time > 0 else 1
                print(f"å¹³å‡å“åº”æ—¶é—´: UI {avg_ui_time:.3f}s, Full {avg_full_time:.3f}s (æå‡ {improvement:.1f}x)")
        
        # å‹ç¼©ç»Ÿè®¡
        compressed_count = sum(1 for t in self.results['trajectory_tests'] + self.results['pr_curve_tests'] if t.get('compressed'))
        total_requests = len(self.results['trajectory_tests']) + len(self.results['pr_curve_tests'])
        if total_requests > 0:
            print(f"HTTPå‹ç¼©: {compressed_count}/{total_requests} è¯·æ±‚å¯ç”¨å‹ç¼©")
        
        print("="*60)


async def main():
    parser = argparse.ArgumentParser(description="éªŒè¯è½¨è¿¹åŠ è½½ä¼˜åŒ–")
    parser.add_argument("--storage-root", required=True, help="å­˜å‚¨æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--base-url", default="http://127.0.0.1:5000", help="APIåŸºç¡€URL")
    parser.add_argument("--max-cases", type=int, default=5, help="æœ€å¤§æµ‹è¯•ç”¨ä¾‹æ•°")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    storage_root = Path(args.storage_root)
    if not storage_root.exists():
        print(f"é”™è¯¯: å­˜å‚¨æ ¹ç›®å½•ä¸å­˜åœ¨: {storage_root}")
        sys.exit(1)
    
    print(f"å¼€å§‹ä¼˜åŒ–éªŒè¯...")
    print(f"å­˜å‚¨æ ¹ç›®å½•: {storage_root}")
    print(f"APIåœ°å€: {args.base_url}")
    print(f"æœ€å¤§æµ‹è¯•ç”¨ä¾‹: {args.max_cases}")
    
    try:
        async with OptimizationValidator(args.base_url) as validator:
            await validator.run_all_tests(str(storage_root), args.max_cases)
            validator.print_summary()
            
        print("\néªŒè¯å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\néªŒè¯è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\néªŒè¯å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
