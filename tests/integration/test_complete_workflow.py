"""
å®Œæ•´å·¥ä½œæµç¨‹é›†æˆæµ‹è¯•

æµ‹è¯•å‰åç«¯é›†æˆçš„å®Œæ•´ç”¨æˆ·å·¥ä½œæµç¨‹ï¼Œç¡®ä¿æ‰€æœ‰æ–°åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚
åŒ…æ‹¬æ•°æ®é›†ç®¡ç†ã€å®éªŒç›‘æ§ã€ç®—æ³•å¯¹æ¯”å’Œæ‰¹é‡æ“ä½œã€‚

Author: VO-Benchmark Team
Version: 1.0.0
"""

import pytest
import asyncio
import time
from typing import List, Dict, Any
import requests
import json

from tests.conftest import TestClient, TestDatabase


class TestCompleteWorkflow:
    """å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•ç±»"""
    
    def setup_method(self):
        """æµ‹è¯•å‰è®¾ç½®"""
        self.base_url = "http://localhost:5000/api/v1"
        self.client = TestClient()
        self.db = TestDatabase()
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        self.db.clean_test_data()
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
        self.test_datasets = self._create_test_datasets()
        
    def teardown_method(self):
        """æµ‹è¯•åæ¸…ç†"""
        self.db.clean_test_data()
    
    def test_complete_user_workflow(self):
        """
        æµ‹è¯•å®Œæ•´çš„ç”¨æˆ·å·¥ä½œæµç¨‹
        
        å·¥ä½œæµç¨‹ï¼š
        1. æµè§ˆå’Œé€‰æ‹©æ•°æ®é›†
        2. æ‰¹é‡åˆ›å»ºå¯¹æ¯”å®éªŒ
        3. ç›‘æ§å®éªŒæ‰§è¡Œ
        4. åˆ†æå’Œå¯¹æ¯”ç»“æœ
        5. å¯¼å‡ºæŠ¥å‘Š
        """
        print("\nğŸš€ å¼€å§‹å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•...")
        
        # æ­¥éª¤1: æ•°æ®é›†ç®¡ç†
        datasets = self._test_dataset_management()
        assert len(datasets) > 0, "åº”è¯¥å‘ç°è‡³å°‘ä¸€ä¸ªæ•°æ®é›†"
        
        # æ­¥éª¤2: æ‰¹é‡åˆ›å»ºå®éªŒ
        experiments = self._test_batch_experiment_creation(datasets[0])
        assert len(experiments) >= 2, "åº”è¯¥åˆ›å»ºè‡³å°‘2ä¸ªå¯¹æ¯”å®éªŒ"
        
        # æ­¥éª¤3: å®éªŒç›‘æ§
        self._test_experiment_monitoring(experiments)
        
        # æ­¥éª¤4: ç®—æ³•å¯¹æ¯”åˆ†æ
        comparison_result = self._test_algorithm_comparison(experiments)
        assert comparison_result is not None, "åº”è¯¥ç”Ÿæˆå¯¹æ¯”åˆ†æç»“æœ"
        
        # æ­¥éª¤5: ç»“æœå¯¼å‡º
        self._test_result_export(comparison_result)
        
        print("âœ… å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡ï¼")
    
    def _test_dataset_management(self) -> List[Dict[str, Any]]:
        """æµ‹è¯•æ•°æ®é›†ç®¡ç†åŠŸèƒ½"""
        print("ğŸ“ æµ‹è¯•æ•°æ®é›†ç®¡ç†...")
        
        # è·å–æ•°æ®é›†åˆ—è¡¨
        response = requests.get(f"{self.base_url}/datasets/")
        assert response.status_code == 200, f"è·å–æ•°æ®é›†åˆ—è¡¨å¤±è´¥: {response.text}"
        
        data = response.json()
        datasets = data.get('datasets', [])
        
        print(f"   å‘ç° {len(datasets)} ä¸ªæ•°æ®é›†")
        
        # éªŒè¯æ•°æ®é›†ç»“æ„
        if datasets:
            dataset = datasets[0]
            required_fields = ['name', 'path', 'type', 'sequences', 'total_frames']
            for field in required_fields:
                assert field in dataset, f"æ•°æ®é›†ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
            
            # æµ‹è¯•æ•°æ®é›†éªŒè¯
            validation_response = requests.post(
                f"{self.base_url}/datasets/validate",
                json={'path': dataset['path']}
            )
            assert validation_response.status_code == 200, "æ•°æ®é›†éªŒè¯å¤±è´¥"
            
            validation_result = validation_response.json()
            print(f"   æ•°æ®é›†éªŒè¯ç»“æœ: {'æœ‰æ•ˆ' if validation_result['valid'] else 'æ— æ•ˆ'}")
        
        return datasets
    
    def _test_batch_experiment_creation(self, dataset: Dict[str, Any]) -> List[str]:
        """æµ‹è¯•æ‰¹é‡å®éªŒåˆ›å»º"""
        print("ğŸ”¬ æµ‹è¯•æ‰¹é‡å®éªŒåˆ›å»º...")
        
        # å‡†å¤‡æ‰¹é‡åˆ›å»ºé…ç½®
        batch_config = {
            'name_template': 'ç®—æ³•å¯¹æ¯”_{algorithm}_{timestamp}',
            'base_config': {
                'dataset_path': dataset['path'],
                'output_dir': '/tmp/test_results',
                'max_features': 1000,
                'sequences': [dataset['sequences'][0]['name']] if dataset['sequences'] else ['test_seq']
            },
            'variations': [
                {'feature_types': ['SIFT']},
                {'feature_types': ['ORB']}
            ],
            'parallel_execution': False
        }
        
        # æ‰§è¡Œæ‰¹é‡åˆ›å»º
        response = requests.post(
            f"{self.base_url}/batch/experiments/create",
            json=batch_config
        )
        assert response.status_code == 200, f"æ‰¹é‡åˆ›å»ºå®éªŒå¤±è´¥: {response.text}"
        
        result = response.json()
        print(f"   æ‰¹é‡åˆ›å»ºç»“æœ: {result['success_count']}/{result['total_count']} æˆåŠŸ")
        
        # æå–å®éªŒID
        experiment_ids = [
            r['target_id'] for r in result['results'] 
            if r['success']
        ]
        
        return experiment_ids
    
    def _test_experiment_monitoring(self, experiment_ids: List[str]):
        """æµ‹è¯•å®éªŒç›‘æ§åŠŸèƒ½"""
        print("ğŸ“Š æµ‹è¯•å®éªŒç›‘æ§...")
        
        for exp_id in experiment_ids[:2]:  # åªæµ‹è¯•å‰ä¸¤ä¸ªå®éªŒ
            # è·å–å®éªŒçŠ¶æ€
            response = requests.get(f"{self.base_url}/monitoring/experiments/{exp_id}")
            
            if response.status_code == 200:
                status = response.json()
                print(f"   å®éªŒ {exp_id}: {status['status']} ({status['overall_progress']*100:.1f}%)")
                
                # éªŒè¯çŠ¶æ€ç»“æ„
                required_fields = ['experiment_id', 'status', 'overall_progress', 'tasks', 'system_metrics']
                for field in required_fields:
                    assert field in status, f"å®éªŒçŠ¶æ€ç¼ºå°‘å­—æ®µ: {field}"
                
                # æµ‹è¯•å®éªŒæ§åˆ¶ï¼ˆå¦‚æœå®éªŒæ­£åœ¨è¿è¡Œï¼‰
                if status['status'] == 'running':
                    control_response = requests.post(
                        f"{self.base_url}/monitoring/experiments/{exp_id}/control",
                        json={'action': 'pause', 'reason': 'æµ‹è¯•æš‚åœ'}
                    )
                    if control_response.status_code == 200:
                        print(f"   æˆåŠŸæš‚åœå®éªŒ {exp_id}")
            else:
                print(f"   å®éªŒ {exp_id} çŠ¶æ€è·å–å¤±è´¥: {response.status_code}")
        
        # æµ‹è¯•ç³»ç»ŸæŒ‡æ ‡
        metrics_response = requests.get(f"{self.base_url}/monitoring/system")
        if metrics_response.status_code == 200:
            metrics = metrics_response.json()
            print(f"   ç³»ç»ŸæŒ‡æ ‡: CPU {metrics['cpu_usage']:.1f}%, å†…å­˜ {metrics['memory_usage']:.1f}%")
    
    def _test_algorithm_comparison(self, experiment_ids: List[str]) -> Dict[str, Any]:
        """æµ‹è¯•ç®—æ³•å¯¹æ¯”åˆ†æ"""
        print("ğŸ“ˆ æµ‹è¯•ç®—æ³•å¯¹æ¯”åˆ†æ...")
        
        # å‡†å¤‡å¯¹æ¯”è¯·æ±‚
        comparison_request = {
            'experiment_ids': experiment_ids,
            'algorithm_names': ['SIFT_STANDARD', 'ORB_STANDARD'],
            'sort_by': 'trajectory_rmse',
            'sort_order': 'asc'
        }
        
        # æ‰§è¡Œå¯¹æ¯”åˆ†æ
        response = requests.post(
            f"{self.base_url}/comparison/analyze",
            json=comparison_request
        )
        
        if response.status_code == 200:
            result = response.json()
            print(f"   å¯¹æ¯”åˆ†æå®Œæˆï¼ŒåŒ…å« {len(result['algorithms'])} ä¸ªç®—æ³•")
            
            # éªŒè¯ç»“æœç»“æ„
            required_fields = ['algorithms', 'statistics', 'recommendations']
            for field in required_fields:
                assert field in result, f"å¯¹æ¯”ç»“æœç¼ºå°‘å­—æ®µ: {field}"
            
            # æ˜¾ç¤ºæœ€ä½³ç®—æ³•
            if result['statistics'].get('best_algorithm'):
                print(f"   æœ€ä½³ç®—æ³•: {result['statistics']['best_algorithm']}")
            
            return result
        else:
            print(f"   å¯¹æ¯”åˆ†æå¤±è´¥: {response.status_code} - {response.text}")
            return None
    
    def _test_result_export(self, comparison_result: Dict[str, Any]):
        """æµ‹è¯•ç»“æœå¯¼å‡ºåŠŸèƒ½"""
        print("ğŸ“„ æµ‹è¯•ç»“æœå¯¼å‡º...")
        
        # æµ‹è¯•ä¸åŒæ ¼å¼çš„å¯¼å‡º
        export_formats = ['csv', 'json']
        
        for format_type in export_formats:
            export_request = {
                'format': format_type,
                'include_charts': False,
                'include_raw_data': True
            }
            
            response = requests.post(
                f"{self.base_url}/comparison/export",
                json=export_request
            )
            
            if response.status_code == 200:
                print(f"   {format_type.upper()} å¯¼å‡ºæˆåŠŸ")
                
                # éªŒè¯å“åº”å¤´
                content_type = response.headers.get('content-type', '')
                if format_type == 'csv':
                    assert 'text/csv' in content_type or 'application/octet-stream' in content_type
                elif format_type == 'json':
                    assert 'application/json' in content_type or 'application/octet-stream' in content_type
            else:
                print(f"   {format_type.upper()} å¯¼å‡ºå¤±è´¥: {response.status_code}")
    
    def _create_test_datasets(self) -> List[Dict[str, Any]]:
        """åˆ›å»ºæµ‹è¯•æ•°æ®é›†"""
        return [
            {
                'name': 'Test TUM Dataset',
                'path': '/tmp/test_tum',
                'type': 'TUM',
                'description': 'æµ‹è¯•ç”¨TUMæ•°æ®é›†',
                'total_frames': 100,
                'sequences': [
                    {
                        'name': 'test_sequence',
                        'path': '/tmp/test_tum/test_sequence',
                        'frame_count': 100,
                        'duration': 10.0,
                        'ground_truth_available': True,
                        'format_valid': True
                    }
                ],
                'format_valid': True
            }
        ]
    
    def test_api_error_handling(self):
        """æµ‹è¯•APIé”™è¯¯å¤„ç†"""
        print("\nğŸš¨ æµ‹è¯•APIé”™è¯¯å¤„ç†...")
        
        # æµ‹è¯•ä¸å­˜åœ¨çš„ç«¯ç‚¹
        response = requests.get(f"{self.base_url}/nonexistent")
        assert response.status_code == 404
        
        # æµ‹è¯•æ— æ•ˆçš„å®éªŒID
        response = requests.get(f"{self.base_url}/monitoring/experiments/invalid_id")
        assert response.status_code == 404
        
        # æµ‹è¯•æ— æ•ˆçš„è¯·æ±‚æ•°æ®
        response = requests.post(
            f"{self.base_url}/batch/experiments/create",
            json={'invalid': 'data'}
        )
        assert response.status_code == 400
        
        print("   APIé”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")
    
    def test_performance_benchmarks(self):
        """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
        print("\nâš¡ æµ‹è¯•æ€§èƒ½åŸºå‡†...")
        
        # æµ‹è¯•APIå“åº”æ—¶é—´
        start_time = time.time()
        response = requests.get(f"{self.base_url}/datasets/")
        response_time = time.time() - start_time
        
        assert response_time < 2.0, f"æ•°æ®é›†åˆ—è¡¨APIå“åº”æ—¶é—´è¿‡é•¿: {response_time:.2f}s"
        print(f"   æ•°æ®é›†åˆ—è¡¨APIå“åº”æ—¶é—´: {response_time:.3f}s")
        
        # æµ‹è¯•å¥åº·æ£€æŸ¥å“åº”æ—¶é—´
        start_time = time.time()
        response = requests.get(f"{self.base_url}/health")
        health_response_time = time.time() - start_time
        
        assert health_response_time < 0.5, f"å¥åº·æ£€æŸ¥å“åº”æ—¶é—´è¿‡é•¿: {health_response_time:.2f}s"
        print(f"   å¥åº·æ£€æŸ¥å“åº”æ—¶é—´: {health_response_time:.3f}s")
    
    def test_concurrent_requests(self):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚å¤„ç†"""
        print("\nğŸ”„ æµ‹è¯•å¹¶å‘è¯·æ±‚å¤„ç†...")
        
        import concurrent.futures
        import threading
        
        def make_request():
            response = requests.get(f"{self.base_url}/health")
            return response.status_code == 200
        
        # å¹¶å‘å‘é€10ä¸ªè¯·æ±‚
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(make_request) for _ in range(10)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        success_count = sum(results)
        assert success_count >= 8, f"å¹¶å‘è¯·æ±‚æˆåŠŸç‡è¿‡ä½: {success_count}/10"
        print(f"   å¹¶å‘è¯·æ±‚æˆåŠŸç‡: {success_count}/10")


if __name__ == "__main__":
    # è¿è¡Œé›†æˆæµ‹è¯•
    test_workflow = TestCompleteWorkflow()
    test_workflow.setup_method()
    
    try:
        test_workflow.test_complete_user_workflow()
        test_workflow.test_api_error_handling()
        test_workflow.test_performance_benchmarks()
        test_workflow.test_concurrent_requests()
        
        print("\nğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"\nâŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        raise
    finally:
        test_workflow.teardown_method()
